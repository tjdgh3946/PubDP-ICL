{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Library import and define methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# !pip install array-to-latex\n",
    "\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "import evaluate \n",
    "import json\n",
    "from semantic_group.jointEM import *\n",
    "import openai\n",
    "import heapq\n",
    "from tqdm import tqdm \n",
    "import os\n",
    "from rouge_score import rouge_scorer\n",
    "metric = evaluate.load(\"rouge\")\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "import random\n",
    "import openai\n",
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_num = 100\n",
    "eps_1 = 0.53\n",
    "eps_3 = 1.25 \n",
    "eps_8 = 2.22 \n",
    "eps_inf = np.inf\n",
    "openai.api_key = \"YOUR OPENAI API KEY\"\n",
    "config = {\n",
    "    \"model\": \"davinci-002\", \n",
    "    \"max_token\": 100, \n",
    "    \"temperature\": 0.02 \n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def selectwith_public1shot(public_example, query, candidates):\n",
    "    public_ref =  public_example.split(\"Dialogue:\")[-2]\n",
    "    prompt = '['\n",
    "    for candidate in candidates:\n",
    "        prompt += candidate + ',\\n'\n",
    "    prompt = prompt[:-2] + ']'\n",
    "    prompt = prompt + '\\nThe summary is:'\n",
    "    query = query + \"\\nPick the most accurate summary for the dialogue with the following summary suggestions:\"\n",
    "    context = \"Dialogue:\\n\"+public_ref + \"Dialogue:\\n\"+query\n",
    "    final_prompt = context + prompt\n",
    "\n",
    "    return final_prompt  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def top_k_indices(counts, k):\n",
    "    # Get the indices of the k largest elements\n",
    "    return heapq.nlargest(k, range(len(counts)), key=lambda i: counts[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calculate_rouge_scores(predictions, references):\n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # List to store ROUGE scores for each prediction-reference pair\n",
    "    scores = []\n",
    "\n",
    "    # Calculate ROUGE scores for each pair\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = scorer.score(ref, pred)  # Reference is the first argument\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def average_rouge_scores(rouge_scores, multi=None):\n",
    "    if not multi: \n",
    "        # Variables to accumulate ROUGE scores for averaging\n",
    "        total_rouge1 = 0\n",
    "        total_rouge2 = 0\n",
    "        total_rougeL = 0\n",
    "        num_examples = len(rouge_scores)\n",
    "        \n",
    "        # Accumulate the F1 scores for each metric from the collection of scores\n",
    "        for score in rouge_scores:\n",
    "            total_rouge1 += score['rouge1'].fmeasure\n",
    "            total_rouge2 += score['rouge2'].fmeasure\n",
    "            total_rougeL += score['rougeL'].fmeasure\n",
    "\n",
    "        # Calculate the average ROUGE scores\n",
    "        avg_rouge1 = total_rouge1 / num_examples\n",
    "        avg_rouge2 = total_rouge2 / num_examples\n",
    "        avg_rougeL = total_rougeL / num_examples\n",
    "        \n",
    "        print(f\"Mean: rouge1: {avg_rouge1}, rouge2: {avg_rouge2}, rougeL:{avg_rougeL}\")\n",
    "        \n",
    "    else:\n",
    "        rouge1 = []\n",
    "        rouge2 = []\n",
    "        rougeL = []\n",
    "        # In this case, rouge_scores is nested array. \n",
    "        for scores in rouge_scores:\n",
    "                    # Variables to accumulate ROUGE scores for averaging\n",
    "            total_rouge1 = 0\n",
    "            total_rouge2 = 0\n",
    "            total_rougeL = 0\n",
    "            num_examples = len(scores)\n",
    "            \n",
    "            # Accumulate the F1 scores for each metric from the collection of scores\n",
    "            for score in scores:\n",
    "                total_rouge1 += score['rouge1'].fmeasure\n",
    "                total_rouge2 += score['rouge2'].fmeasure\n",
    "                total_rougeL += score['rougeL'].fmeasure\n",
    "\n",
    "            # Calculate the average ROUGE scores\n",
    "            avg_rouge1 = total_rouge1 / num_examples\n",
    "            avg_rouge2 = total_rouge2 / num_examples\n",
    "            avg_rougeL = total_rougeL / num_examples\n",
    "            rouge1.append(avg_rouge1)\n",
    "            rouge2.append(avg_rouge2)\n",
    "            rougeL.append(avg_rougeL)\n",
    "        rouge1_avg, rouge2_avg, rougeL_avg = np.mean(rouge1)*100, np.mean(rouge2)*100, np.mean(rougeL)*100\n",
    "        rouge1_std, rouge2_std, rougeL_std = np.std(rouge1)*100, np.std(rouge2)*100, np.std(rougeL)*100\n",
    "\n",
    "        print(f\"Mean: rouge1: {rouge1_avg:.3f}, rouge2: {rouge2_avg:.3f}, rougeL:{rougeL_avg:.3f}\")\n",
    "        print(f\"Std: rouge1: {rouge1_std:.3f}, rouge2: {rouge2_std:.3f}, rougeL:{rougeL_std:.3f}\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Histogram Load "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def add_countfield(histograms): \n",
    "    for i, ensemble in enumerate(histograms):\n",
    "        count = []\n",
    "        for k,v in ensemble.items(): \n",
    "            if k not in ('query', 'reference', 'count'): \n",
    "                count.append(len(ensemble[k]))\n",
    "        histograms[i]['count'] = count \n",
    "    return histograms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def histogram_extract(histograms, tag=None):\n",
    "    new_histograms =[] \n",
    "    for i, ensemble in enumerate(histograms):\n",
    "        new_histogram ={}\n",
    "        for k,v in ensemble.items():\n",
    "            if k not in ('query', 'reference', 'count'):\n",
    "                new_hist =[]  \n",
    "                for elem in v:\n",
    "                    if elem[1] == tag:\n",
    "                        new_hist.append(elem)\n",
    "                if len(new_hist) > 0:\n",
    "                    new_histogram[k] = new_hist \n",
    "\n",
    "            else:\n",
    "                new_histogram[k] = v  \n",
    "        new_histograms.append(new_histogram)\n",
    "        \n",
    "    return new_histograms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_histogram(path):\n",
    "    with open(path ,'r') as f:\n",
    "        file = json.load(f)\n",
    "        histograms =[]\n",
    "        for line in file:\n",
    "            histograms.append(line)\n",
    "    return histograms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path = \"semantic_group/Summarization_davinci-002_100way-4shot_aggregate_090.json\"\n",
    "private_path = \"semantic_group/Summarization_davinci-002_100way-private_090.json\"\n",
    "histograms = load_histogram(path)\n",
    "private_histograms = load_histogram(private_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output_private = \"output/Summarization_davinci-002_100way-4shot_private.json\"\n",
    "output_public = \"output/Summarization_davinci-002_100way-4shot_public.json\"\n",
    "with open(output_private, 'r') as f:\n",
    "    file_private = json.load(f)\n",
    "with open(output_public, 'r') as f:\n",
    "    file_public = json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KSA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get predictions from the json file\n",
    "trials = 1\n",
    "ds_size = 100 \n",
    "ensemble= 100 \n",
    "private_only = True \n",
    "eps = eps_1\n",
    "final_preds_group =[]\n",
    "references_group = []\n",
    "for trial in range(trials):\n",
    "     random.seed(2*trial)\n",
    "     np.random.seed(2*trial)\n",
    "     final_preds = []\n",
    "     references =[]\n",
    "     for i in tqdm(range(ds_size)):\n",
    "          all_tokens = {} # key: token, value: count\n",
    "          for j in range(ensemble):\n",
    "               sentence_private, sentence_public = file_private[i*ensemble+j]['prediction'], file_public[i*ensemble+j]['prediction']\n",
    "               tokens_private = nltk.word_tokenize(sentence_private)\n",
    "               tokens_public = nltk.word_tokenize(sentence_public)\n",
    "               tokens_group = (tokens_private, tokens_public) if not private_only else [tokens_private]\n",
    "               for tokens in tokens_group:\n",
    "                    onegrams = set(ngrams(tokens, 1))\n",
    "                    # onegrams = set(onegrams)\n",
    "                    # making onegrams a set to avoid duplicate tokens\n",
    "                    for token in onegrams:\n",
    "                         # only add one gram for one sentence\n",
    "                         if token in all_tokens:\n",
    "                              all_tokens[token] += 1\n",
    "                         else:\n",
    "                              all_tokens[token] = 1\n",
    "          all_tokens_sorted = sorted(all_tokens.items(), key=lambda x: x[1], reverse=True)\n",
    "          # ignore those non-words tokens\n",
    "          filtered_tokens = {}\n",
    "          for token, count in all_tokens_sorted:\n",
    "               if not all(word in string.punctuation for word in token) and token[0] not in stopword_set:\n",
    "                    filtered_tokens[token] = count\n",
    "          filtered_tokens_sorted = sorted(filtered_tokens.items(), key=lambda x: x[1], reverse=True)\n",
    "          item_counts = np.array([count for token, count in filtered_tokens_sorted])\n",
    "          joint_out = joint(item_counts, k=min(10, len(item_counts)), epsilon=eps, neighbor_type=1)\n",
    "          filtered_tokens = np.array(filtered_tokens_sorted, dtype=object)[joint_out]\n",
    "          filtered_tokens = [token_tuple[0][0] for token_tuple in filtered_tokens]\n",
    "          prompt = '['\n",
    "          for token in filtered_tokens:\n",
    "               prompt += token + ', '\n",
    "          prompt = prompt[:-2] + ']'\n",
    "          prompt = prompt + '\\nThe summary is:'\n",
    "          public_ref =  file_public[i*ensemble+j]['prompt'].split(\"Dialogue:\")[-2]\n",
    "          query =  file_public[i*ensemble+j]['prompt'].split(\"Dialogue:\")[-1]\n",
    "          zero_shot_sentence = \"Dialogue:\\n\"+public_ref + \"Dialogue:\\n\"+query.replace('Summarize the above dialogue:','Summarize the above dialogue with the following word suggestions ranked by their frequency from high to low:') \n",
    "          final_prompt = zero_shot_sentence  + prompt\n",
    "          response = openai.Completion.create(\n",
    "                    engine=config['model'],\n",
    "                    prompt=final_prompt,\n",
    "                    max_tokens=config['max_token'],\n",
    "                    temperature=config['temperature'], \n",
    "                    stop=\"\\n\" \n",
    "               )['choices'][0]['text']\n",
    "          references.append(file_private[i*ensemble+j]['reference'])\n",
    "          final_preds.append(response)\n",
    "     final_preds_group.append(final_preds)\n",
    "     references_group.append(references)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = [] \n",
    "for i in range(trials):\n",
    "    scores.append(calculate_rouge_scores(final_preds_group[i], references_group[i]))\n",
    "average_rouge_scores(scores, multi=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction (Ours)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trials = 3\n",
    "k=3\n",
    "ds_size = 100\n",
    "ensemble= 100 \n",
    "final_preds_group =[]\n",
    "references_group = []\n",
    "eps = eps_8\n",
    "for trial in range(trials):\n",
    "    random.seed(2*trial)\n",
    "    np.random.seed(2*trial)\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for i in tqdm(range(test_num)): \n",
    "        # private top-k selection \n",
    "        cnts = np.array(histograms[i]['count'])\n",
    "        joint_out = joint(cnts, k=k, epsilon=eps, neighbor_type=1)\n",
    "        #joint_out = top_k_indices(cnts, k=k)\n",
    "        candidates = [] \n",
    "        for idx in joint_out:\n",
    "            keys = list(histograms[i].keys())[:-3]\n",
    "            target_key = keys[idx]\n",
    "            candidates.append(histograms[i][target_key][0][0])          # pick representative \n",
    "\n",
    "        query = histograms[i]['query']\n",
    "        reference = histograms[i]['reference']\n",
    "        prompt = selectwith_public1shot(file_public[i*ensemble]['prompt'], query, candidates)\n",
    "        response = openai.Completion.create(\n",
    "                    engine=config['model'],\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=config['max_token'],\n",
    "                    temperature=config['temperature'], \n",
    "                    stop=\"\\n\" \n",
    "                )\n",
    "        prediction = response['choices'][0]['text']\n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "    final_preds_group.append(predictions)\n",
    "    references_group.append(references)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = [] \n",
    "for i in range(trials):\n",
    "    scores.append(calculate_rouge_scores(final_preds_group[i], references_group[i]))\n",
    "average_rouge_scores(scores, multi=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Direct top-1 selection "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trials = 5\n",
    "ds_size = 100 \n",
    "final_preds_group =[]\n",
    "references_group = []\n",
    "eps=eps_inf\n",
    "k=6\n",
    "for trial in range(trials):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for i in tqdm(range(test_num)): \n",
    "        # private top-k selection \n",
    "        cnts = np.array(histograms[i]['count'])\n",
    "        if eps==np.inf:\n",
    "            joint_out = top_k_indices(cnts, k=k)\n",
    "        else:\n",
    "            joint_out = joint(cnts, k=k, epsilon=eps, neighbor_type=1)\n",
    "        candidates = [] \n",
    "        for idx in joint_out:\n",
    "            keys = list(histograms[i].keys())[:-3]\n",
    "            target_key = keys[joint_out[0]]\n",
    "            candidates.append(histograms[i][target_key][0][0])          # pick representative \n",
    "        prediction = candidates[0]\n",
    "        reference = histograms[i]['reference']\n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "    final_preds_group.append(predictions)\n",
    "    references_group.append(references)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = [] \n",
    "for i in range(trials):\n",
    "    scores.append(calculate_rouge_scores(final_preds_group[i], references_group[i]))\n",
    "average_rouge_scores(scores, multi=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4-shot prediction "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path = \"output/samsum_davinci-002_5way-0shot_private.json\"\n",
    "trials=5\n",
    "num_ensemble = 100\n",
    "ensemble=5 \n",
    "final_preds_group =[]\n",
    "references_group = []\n",
    "with open(path, 'r') as f:\n",
    "    file = json.load(f)\n",
    "num_examples = len(file)\n",
    "\n",
    "for i in range(trials):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for j in range(0, num_examples, ensemble):\n",
    "        example = file[j+i] \n",
    "        prediction, reference = example[\"prediction\"], example[\"reference\"]\n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "    final_preds_group.append(predictions)\n",
    "    references_group.append(references)    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = [] \n",
    "for i in range(trials):\n",
    "    scores.append(calculate_rouge_scores(final_preds_group[i], references_group[i]))\n",
    "average_rouge_scores(scores, multi=True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.14 64-bit ('DPICL': conda)"
  },
  "interpreter": {
   "hash": "a0568f393577d1bed95edec3cdb8e054e4492b9901216a73120ac0d92b2d7919"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}