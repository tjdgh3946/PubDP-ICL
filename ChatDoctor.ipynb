{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# !pip install array-to-latex\n",
    "\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "import evaluate \n",
    "import json\n",
    "from semantic_group.jointEM import *\n",
    "import openai\n",
    "import heapq\n",
    "from tqdm import tqdm \n",
    "import os\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "metric = evaluate.load(\"rouge\")\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "import random\n",
    "import openai\n",
    "import evaluate\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from difflib import SequenceMatcher\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from openicl import PromptTemplate \n",
    "template_map = {\n",
    "    \"samsum\": [\n",
    "        '</E>Dialogue:\"\\n </dialogue>\" \\nSummarize the above dialogue: </summary>',\n",
    "        {\"dialogue\": \"</dialogue>\", \"summary\": \"</summary\"},\n",
    "    ],\n",
    "    \"virattt/financial-qa-10K\": [\n",
    "        '</E>Context: \"\\n </context>\"\\n\\nQuestion: </question>\\nAnswer: </answer>',\n",
    "        {\"context\": \"</context>\", \"question\": \"</question>\", \"answer\": \"</answer>\"},\n",
    "    ],\n",
    "    \"Malikeh1375/medical-question-answering-datasets\": [\n",
    "        \"</E>Question: </question>\\n\\nAnswer: </answer>\\n\",\n",
    "        {\"input\": \"</question>\", \"output\": \"</answer>\"},\n",
    "    ],\n",
    "}\n",
    "data_name = \"Malikeh1375/medical-question-answering-datasets\"\n",
    "template = PromptTemplate(\n",
    "    template_map[data_name][0],\n",
    "    template_map[data_name][1],\n",
    "    ice_token=\"</E>\",\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_num = 100\n",
    "eps_half = 0.13\n",
    "eps_1 = 0.23\n",
    "eps_3 = 0.63\n",
    "eps_5 = 0.93\n",
    "eps_8 = 1.32\n",
    "eps_12 = 1.92\n",
    "eps_inf = np.inf\n",
    "openai.api_key = \"YOUR OPENAI API KEY\"\n",
    "config = {\n",
    "    \"model\": \"gpt-3.5-turbo\", \n",
    "    \"max_token\": 200, \n",
    "    \"temperature\": 0.02 \n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def selectwith_public1shot(public_example, query, candidates):\n",
    "    public_ref =  public_example.split(\"Question:\")[-2]\n",
    "    prompt = '['\n",
    "    for candidate in candidates:\n",
    "        prompt += candidate + ',\\n'\n",
    "    prompt = prompt[:-2] + ']'\n",
    "    prompt = prompt + '\\nThe answer is:'\n",
    "    query = query + \"\\nPick the most accurate answer for the question with the following answer candidates ranked by their freqeuncy from high to low: \"\n",
    "    context = \"Question:\\n\"+public_ref + \"Question:\\n\"+query\n",
    "    final_prompt = context + prompt\n",
    "\n",
    "    return final_prompt  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def top_k_indices(counts, k):\n",
    "    # Get the indices of the k largest elements\n",
    "    return heapq.nlargest(k, range(len(counts)), key=lambda i: counts[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_all_scores(predictions, references):\n",
    "    \"\"\"Calculate BLEU, METEOR, and ROUGE-1 scores for each prediction-reference pair.\"\"\"\n",
    "    # Initialize the scorer for ROUGE-1\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "    \n",
    "    # Smoothing function for BLEU\n",
    "    smoothie = SmoothingFunction().method4\n",
    "\n",
    "    # Lists to store the results for each metric\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    rouge1_scores = []\n",
    "\n",
    "    # Loop through each prediction-reference pair\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # Tokenize both prediction and reference\n",
    "        pred_tokens = word_tokenize(pred)\n",
    "        ref_tokens = word_tokenize(ref)\n",
    "\n",
    "        # BLEU score calculation\n",
    "        bleu_score = sentence_bleu([ref_tokens], pred_tokens, weights=(1.0, 0.0, 0.0, 0.0), smoothing_function=smoothie)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        # METEOR score calculation (expects strings or lists of tokens)\n",
    "        meteor_score_value = meteor_score([ref_tokens], pred_tokens)\n",
    "        meteor_scores.append(meteor_score_value)\n",
    "\n",
    "        # ROUGE-1 score calculation\n",
    "        rouge_score = rouge_scorer_instance.score(ref, pred)\n",
    "        rouge1_score = rouge_score['rouge1'].fmeasure\n",
    "        rouge1_scores.append(rouge1_score)\n",
    "\n",
    "    # Return the results as a dictionary of lists\n",
    "    return {\n",
    "        'BLEU': np.mean(bleu_scores),\n",
    "        'METEOR': np.mean(meteor_scores),\n",
    "        'ROUGE1': np.mean(rouge1_scores)\n",
    "    }\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calculate_mean_and_std_from_dict_scores(score_list):\n",
    "    \"\"\"\n",
    "    Calculate the mean and standard deviation for BLEU, Levenshtein similarity, and ROUGE-1 scores\n",
    "    from a list of dictionaries where each dictionary contains {'BLEU': value, 'Levenshtein': value, 'ROUGE1': value}.\n",
    "    \n",
    "    Parameters:\n",
    "    score_list (list): A list of dictionaries, where each dictionary contains 'BLEU', 'Levenshtein', 'ROUGE1' scores.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing the mean and standard deviation for BLEU, Levenshtein, and ROUGE-1 scores.\n",
    "    \"\"\"\n",
    "    # Unpack the scores into separate lists for BLEU, Levenshtein, and ROUGE-1\n",
    "    bleu_scores = [scores['BLEU'] for scores in score_list]\n",
    "    meteor_scores = [scores['METEOR'] for scores in score_list]\n",
    "    rouge1_scores = [scores['ROUGE1'] for scores in score_list]\n",
    "    \n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    results = {}\n",
    "    \n",
    "    # BLEU\n",
    "    bleu_mean = np.mean(bleu_scores) * 100\n",
    "    bleu_std = np.std(bleu_scores) * 100\n",
    "    results['bleu'] = {'mean': bleu_mean, 'std': bleu_std}\n",
    "    \n",
    "    # Levenshtein\n",
    "    meteor_mean = np.mean(meteor_scores) * 100\n",
    "    meteor_std = np.std(meteor_scores) * 100\n",
    "    results['meteor'] = {'mean': meteor_mean, 'std': meteor_std}\n",
    "    \n",
    "    # ROUGE-1\n",
    "    rouge1_mean = np.mean(rouge1_scores) * 100\n",
    "    rouge1_std = np.std(rouge1_scores) * 100\n",
    "    results['rouge1'] = {'mean': rouge1_mean, 'std': rouge1_std}\n",
    "    \n",
    "    return results\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_histogram(path):\n",
    "    with open(path ,'r') as f:\n",
    "        file = json.load(f)\n",
    "        histograms =[]\n",
    "        for line in file:\n",
    "            histograms.append(line)\n",
    "    return histograms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path = \"semantic_group/0.95_medical-question-answering-datasets_chatdoctor_icliniq_gpt-3.5-turbo_100way-4shot_aggregate.json\"\n",
    "#path = \"/home/jsh/workspace/DP-ICL/Generate_seperate_reponses/long_generation/semantic_group/oodpublic_0.92_medical-question-answering-datasets_chatdoctor_icliniq_gpt-3.5-turbo_100way-4shot_aggregate.json\"\n",
    "private_path = \"semantic_group/0.95_medical-question-answering-datasets_chatdoctor_icliniq_gpt-3.5-turbo_100way-4shot_private.json\"\n",
    "histograms = load_histogram(path)\n",
    "private_histograms = load_histogram(private_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output_private = \"/home/jsh/workspace/DP-ICL/Generate_seperate_reponses/long_generation/output/medical-question-answering-datasets_chatdoctor_icliniq_gpt-3.5-turbo_100way-4shot_private.json\"\n",
    "output_public = \"/home/jsh/workspace/DP-ICL/Generate_seperate_reponses/long_generation/output/medical-question-answering-datasets_chatdoctor_icliniq_gpt-3.5-turbo_100way-4shot_public.json\"\n",
    "#output_public = \"output/ood_medical-question-answering-datasets_chatdoctor_healthcaremagic_gpt-3.5-turbo_100way-4shot_public.json\"\n",
    "with open(output_private, 'r') as f:\n",
    "    file_private = json.load(f)\n",
    "with open(output_public, 'r') as f:\n",
    "    file_public = json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KSA Baseline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get predictions from the json file\n",
    "trials = 1\n",
    "ds_size = 100 \n",
    "ensemble= 100 \n",
    "private_only = True      \n",
    "eps = eps_inf\n",
    "k=40  \n",
    "final_preds_group =[]\n",
    "references_group = []\n",
    "print(f\"eps={eps}, k={k}, trials={trials}\")\n",
    "for i in range(trials):\n",
    "     random.seed(2*i)\n",
    "     np.random.seed(2*i)\n",
    "     final_preds = []\n",
    "     references =[]\n",
    "     for i in tqdm(range(ds_size)):\n",
    "          all_tokens = {} # key: token, value: count\n",
    "          for j in range(ensemble):\n",
    "               sentence_private, sentence_public = file_private[i*ensemble+j]['prediction'], file_public[i*ensemble+j]['prediction']\n",
    "               tokens_private = nltk.word_tokenize(sentence_private)\n",
    "               tokens_public = nltk.word_tokenize(sentence_public)\n",
    "               tokens_group = (tokens_private, tokens_public) if not private_only else [tokens_private]\n",
    "               for tokens in tokens_group:\n",
    "                    onegrams = set(ngrams(tokens, 1))\n",
    "                    # onegrams = set(onegrams)\n",
    "                    # making onegrams a set to avoid duplicate tokens\n",
    "                    for token in onegrams:\n",
    "                         # only add one gram for one sentence\n",
    "                         if token in all_tokens:\n",
    "                              all_tokens[token] += 1\n",
    "                         else:\n",
    "                              all_tokens[token] = 1\n",
    "          all_tokens_sorted = sorted(all_tokens.items(), key=lambda x: x[1], reverse=True)\n",
    "          # ignore those non-words tokens\n",
    "          filtered_tokens = {}\n",
    "          for token, count in all_tokens_sorted:\n",
    "               if not all(word in string.punctuation for word in token) and token[0] not in stopword_set:\n",
    "                    filtered_tokens[token] = count\n",
    "          filtered_tokens_sorted = sorted(filtered_tokens.items(), key=lambda x: x[1], reverse=True)\n",
    "          item_counts = np.array([count for token, count in filtered_tokens_sorted])\n",
    "          k=min(k, len(item_counts))\n",
    "          if eps==np.inf:\n",
    "               joint_out = top_k_indices(item_counts, k)\n",
    "          else:\n",
    "               joint_out = joint(item_counts, k, epsilon=eps, neighbor_type=1)\n",
    "          filtered_tokens = np.array(filtered_tokens_sorted, dtype=object)[joint_out]\n",
    "          filtered_tokens = [token_tuple[0][0] for token_tuple in filtered_tokens]\n",
    "          prompt = '['\n",
    "          for token in filtered_tokens:\n",
    "               prompt += token + ', '\n",
    "          prompt = prompt[:-2] + ']'\n",
    "          prompt = prompt + '\\nThe answer is:'\n",
    "          public_ref =  file_public[i*ensemble+j]['prompt'].split(\"Question:\")[-2]\n",
    "          query =  file_public[i*ensemble+j]['prompt'].split(\"Question:\")[-1]\n",
    "          zero_shot_sentence = \"Question:\\n\"+public_ref + \"Question:\\n\"+query.replace('Answer:','Answer the above question with the following word suggestions ranked by their frequency from high to low:') \n",
    "          final_prompt = zero_shot_sentence  + prompt\n",
    "          response = openai.ChatCompletion.create(\n",
    "          model=config[\"model\"],  # Use \"gpt-3.5-turbo\" or your chosen model\n",
    "          messages=[\n",
    "               {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": final_prompt,\n",
    "               },  # The prompt provided by the user\n",
    "          ],\n",
    "          max_tokens=config[\"max_token\"],\n",
    "          temperature=config[\"temperature\"],\n",
    "          stop=[\"\\n\"],\n",
    "          )[\"choices\"][0][\"message\"][\"content\"]\n",
    "          references.append(file_private[i*ensemble+j]['reference'])\n",
    "          final_preds.append(response)\n",
    "     final_preds_group.append(final_preds)\n",
    "     references_group.append(references)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = [] \n",
    "for i in range(trials):\n",
    "    scores.append(calculate_all_scores(final_preds_group[i], references_group[i]))\n",
    "calculate_mean_and_std_from_dict_scores(scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SGA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trials = 1\n",
    "k= 6\n",
    "ds_size = 100\n",
    "ensemble= 100 \n",
    "eps=eps_inf \n",
    "eps_over_query = [eps_8]\n",
    "test_num=1\n",
    "#k_value = [2,3,5,6,8]\n",
    "#histograms = private_histograms\n",
    "for eps in eps_over_query:\n",
    "    final_preds_group =[]\n",
    "    references_group = []\n",
    "    print(f\"eps={eps}, k={k}, trials={trials}\")\n",
    "    for i in range(trials):\n",
    "        random.seed(2*i)\n",
    "        np.random.seed(2*i)\n",
    "        predictions = []\n",
    "        references = []\n",
    "        for j in tqdm(range(test_num)): \n",
    "            # private top-k selection \n",
    "            cnts = np.array(histograms[j]['count'])\n",
    "            top_k = min(k,len(cnts))\n",
    "            if eps==np.inf:\n",
    "                joint_out = top_k_indices(cnts, top_k)\n",
    "            else:\n",
    "                joint_out = joint(cnts, k=top_k, epsilon=eps, neighbor_type=1)\n",
    "            candidates = [] \n",
    "            for idx in joint_out:\n",
    "                keys = list(histograms[j].keys())[:-3]\n",
    "                target_key = keys[idx]\n",
    "                candidates.append(histograms[j][target_key][0][0])          # pick representative \n",
    "\n",
    "            query = histograms[j]['query']\n",
    "            reference = histograms[j]['reference']\n",
    "            if k>1:\n",
    "                prompt = selectwith_public1shot(file_public[j*ensemble]['prompt'], query, candidates)\n",
    "                response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",  # Use \"gpt-3.5-turbo\" or your chosen model\n",
    "                messages=[\n",
    "                    {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt,\n",
    "                    },  # The prompt provided by the user\n",
    "                ],\n",
    "                max_tokens=200,\n",
    "                temperature=0.02,\n",
    "                stop=[\"\\n\"],\n",
    "                )[\"choices\"][0][\"message\"][\"content\"]\n",
    "            else:\n",
    "                response = candidates[0]    # Direct top-1 \n",
    "            prediction = response\n",
    "            predictions.append(prediction)\n",
    "            references.append(reference)\n",
    "        final_preds_group.append(predictions)\n",
    "        references_group.append(references)\n",
    "    scores = [] \n",
    "    for i in range(trials):\n",
    "        scores.append(calculate_all_scores(final_preds_group[i], references_group[i]))\n",
    "    results = calculate_mean_and_std_from_dict_scores(scores)\n",
    "    print(results)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = [] \n",
    "for i in range(trials):\n",
    "    scores.append(calculate_all_scores(final_preds_group[i], references_group[i]))\n",
    "calculate_mean_and_std_from_dict_scores(scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "query\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Direct top-1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "references_group[0][0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trials = 1\n",
    "ds_size = 100 \n",
    "final_preds_group =[]\n",
    "references_group = []\n",
    "k=3\n",
    "eps=eps_1\n",
    "print(f\"eps={eps}, trials={trials}\")\n",
    "for trial in range(trials):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for j in tqdm(range(test_num)): \n",
    "        # Private top-1 selection \n",
    "        cnts = np.array(histograms[j]['count'])\n",
    "        k = min(k,len(cnts))\n",
    "        if eps==np.inf:\n",
    "            joint_out = top_k_indices(cnts, k=k)\n",
    "        else:\n",
    "            joint_out = joint(cnts, k=k, epsilon=eps, neighbor_type=1)\n",
    "        keys = list(histograms[j].keys())[:-3]\n",
    "        target_key = keys[joint_out[0]]\n",
    "        prediction = histograms[j][target_key][0][0]          # pick representative \n",
    "        reference = histograms[j]['reference']\n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "    final_preds_group.append(predictions)\n",
    "    references_group.append(references)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = [] \n",
    "for i in range(trials):\n",
    "    scores.append(calculate_all_scores(final_preds_group[i], references_group[i]))\n",
    "calculate_mean_and_std_from_dict_scores(scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_preds_group[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4-shot ($\\varepsilon=\\infty$)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path = \"output/medical-question-answering-datasets_chatdoctor_icliniq_gpt-3.5-turbo_100way-4shot_private.json\"\n",
    "trials=5\n",
    "num_ensemble = 100 \n",
    "final_preds_group =[]\n",
    "references_group = []\n",
    "with open(path, 'r') as f:\n",
    "    file = json.load(f)\n",
    "num_examples = len(file)\n",
    "\n",
    "for i in range(trials):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for j in range(0, num_examples, ensemble):\n",
    "        example = file[j+i] \n",
    "        prediction, reference = example[\"prediction\"], example[\"reference\"]\n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "    final_preds_group.append(predictions)\n",
    "    references_group.append(references)    \n",
    "        \n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = [] \n",
    "for i in range(trials):\n",
    "    scores.append(calculate_all_scores(final_preds_group[i], references_group[i]))\n",
    "calculate_mean_and_std_from_dict_scores(scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0-shot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path = \"output/medical-question-answering-datasets_chatdoctor_icliniq_gpt-3.5-turbo_5way-0shot_private.json\"\n",
    "trials=5\n",
    "num_ensmble = 5 \n",
    "with open(path , 'r') as f:\n",
    "    file = json.load(f)\n",
    "num_examples = len(file)\n",
    "final_preds_group =[]\n",
    "references_group = []\n",
    "for i in range(trials):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for j in range(0, num_examples, ensemble):\n",
    "        example = file[j+i] \n",
    "        prediction, reference = example[\"prediction\"], example[\"reference\"]\n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "    final_preds_group.append(predictions)\n",
    "    references_group.append(references)   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = [] \n",
    "for i in range(trials):\n",
    "    scores.append(calculate_all_scores(final_preds_group[i], references_group[i]))\n",
    "calculate_mean_and_std_from_dict_scores(scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### With OOD Public "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output_public = \"output/ood_medical-question-answering-datasets_chatdoctor_healthcaremagic_gpt-3.5-turbo_100way-4shot_public.json\"\n",
    "with open(output_public, 'r') as f:\n",
    "    file_public = json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path = \"semantic_group/oodpublic_0.92_medical-question-answering-datasets_chatdoctor_icliniq_gpt-3.5-turbo_100way-4shot_aggregate.json\"\n",
    "private_path = \"semantic_group/oodpublic_0.92_medical-question-answering-datasets_chatdoctor_icliniq_gpt-3.5-turbo_100way-4shot_aggregate.json\"\n",
    "histograms = load_histogram(path)\n",
    "private_histograms = load_histogram(private_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trials = 1\n",
    "k=2\n",
    "ds_size = 100\n",
    "ensemble= 100 \n",
    "final_preds_group =[]\n",
    "references_group = []\n",
    "eps=0.01\n",
    "private_only = False \n",
    "histograms = private_histograms if private_only else histograms\n",
    "print(f\"eps={eps}, private={private_only}, trials={trials}\")\n",
    "for i in range(trials):\n",
    "    random.seed(2*i)\n",
    "    np.random.seed(2*i)\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for j in tqdm(range(test_num)): \n",
    "        # private top-k selection \n",
    "        cnts = np.array(histograms[j]['count'])\n",
    "        top_k = min(k,len(cnts))\n",
    "        if eps==np.inf:\n",
    "            joint_out = top_k_indices(cnts, top_k)\n",
    "        else:\n",
    "            joint_out = joint(cnts, k=top_k, epsilon=eps, neighbor_type=1)\n",
    "        candidates = [] \n",
    "        for idx in joint_out:\n",
    "            keys = list(histograms[j].keys())[:-3]\n",
    "            target_key = keys[idx]\n",
    "            candidates.append(histograms[j][target_key][0][0])          # pick representative \n",
    "\n",
    "        query = histograms[j]['query']\n",
    "        reference = histograms[j]['reference']\n",
    "        prompt = selectwith_public1shot(file_public[j*ensemble]['prompt'], query, candidates)\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "          model=\"gpt-3.5-turbo\",  # Use \"gpt-3.5-turbo\" or your chosen model\n",
    "          messages=[\n",
    "               {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "               },  # The prompt provided by the user\n",
    "          ],\n",
    "          max_tokens=200,\n",
    "          temperature=0.02,\n",
    "          stop=[\"\\n\"],\n",
    "          )[\"choices\"][0][\"message\"][\"content\"]\n",
    "        prediction = response\n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "    final_preds_group.append(predictions)\n",
    "    references_group.append(references)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = [] \n",
    "for i in range(trials):\n",
    "    scores.append(calculate_all_scores(final_preds_group[i], references_group[i]))\n",
    "calculate_mean_and_std_from_dict_scores(scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparmeter analysis (#ensemble, #quries)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# e=(10,30,50,75,100) q=(1, 10, 100, 1000, 10000)\n",
    "eps_3_10 = 4.29 \n",
    "eps_3_25 = 1.84\n",
    "eps_3_50 = 1.15\n",
    "eps_3_100 = 0.63"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.14 64-bit ('DPICL': conda)"
  },
  "interpreter": {
   "hash": "a0568f393577d1bed95edec3cdb8e054e4492b9901216a73120ac0d92b2d7919"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}